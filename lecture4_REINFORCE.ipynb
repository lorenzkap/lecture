{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lorenzkap/lecture/blob/main/lecture4_REINFORCE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NO5SXdOLz2Ly"
      },
      "source": [
        "### Introducing Gymnasium and CartPole\n",
        "\n",
        "\n",
        "The **Gymnasium** library provides a wide range of pre-built environments designed for benchmarking RL algorithms. This lets us focus on the agent's learning logic.\n",
        "\n",
        "We'll use a classic Gymnasium environment called **CartPole-v1**.\n",
        "\n",
        "https://gymnasium.farama.org/environments/classic_control/cart_pole/\n",
        "\n",
        "*   **Goal:** Balance a vertical pole on top of a cart that moves along a track.\n",
        "*   **Observation Space (State):** What the agent sees. It's a vector of 4 continuous values:\n",
        "    1.  Cart Position\n",
        "    2.  Cart Velocity\n",
        "    3.  Pole Angle (0 is upright)\n",
        "    4.  Pole Angular Velocity (how fast it's tipping)\n",
        "*   **Action Space:** What the agent can do. It's discrete with 2 actions:\n",
        "    *   0: Push cart Left\n",
        "    *   1: Push cart Right\n",
        "*   **Reward:** The agent receives a +1 reward for every time step the pole remains upright within certain angle limits and the cart stays within track boundaries.\n",
        "*   **Episode Termination:** An episode ends if:\n",
        "    *   The pole angle exceeds a threshold (pole falls).\n",
        "    *   The cart position exceeds boundaries (cart goes off track).\n",
        "    *   A time limit (500 steps for v1) is reached (truncated).\n",
        "*   **Objective:** Maximize the total reward by keeping the pole balanced for as long as possible (up to 500 steps).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "8ek8t4ES6KjV"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def train_agent_cartpole(episodes=1000, hidden_size=64, lr=1e-3, gamma=0.99, print_every=50, save_path=\"best_dqn_agent.pth\"):\n",
        "    env = gym.make('CartPole-v1')\n",
        "    state_size = env.observation_space.shape[0]\n",
        "    action_size = env.action_space.n\n",
        "\n",
        "    # Instantiate agent and move to device\n",
        "    agent = DQNAgent(state_size, action_size, hidden_size, lr, buffer_size=10000, batch_size=64, gamma=gamma, tau=0.005, epsilon_start=1.0, epsilon_end=0.01, epsilon_decay=0.995).to(device)\n",
        "\n",
        "    episode_rewards = []\n",
        "    from collections import deque\n",
        "    recent_rewards = deque(maxlen=print_every)\n",
        "\n",
        "    best_avg_reward = -float('inf')\n",
        "\n",
        "    for episode in range(episodes):\n",
        "        state_np, info = env.reset()\n",
        "        total_reward = 0\n",
        "        terminated = False\n",
        "        truncated = False\n",
        "\n",
        "        for t in range(env.spec.max_episode_steps):\n",
        "            action_int = agent.get_action(state_np)\n",
        "\n",
        "            next_state_np, reward, terminated, truncated, info = env.step(action_int)\n",
        "\n",
        "            # Store experience\n",
        "            done = terminated or truncated\n",
        "            agent.store_experience(state_np, action_int, reward, next_state_np, done)\n",
        "\n",
        "            # Train\n",
        "            loss = agent.train()\n",
        "\n",
        "            state_np = next_state_np\n",
        "            total_reward += reward\n",
        "\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        episode_rewards.append(total_reward)\n",
        "        recent_rewards.append(total_reward)\n",
        "        current_avg_reward = np.mean(recent_rewards)\n",
        "\n",
        "        if current_avg_reward > best_avg_reward:\n",
        "            best_avg_reward = current_avg_reward\n",
        "            torch.save(agent.q_network.state_dict(), save_path)\n",
        "\n",
        "        if (episode + 1) % print_every == 0:\n",
        "            print(f\"Episode {episode + 1}/{episodes} | Avg Reward (Last {len(recent_rewards)}): {current_avg_reward:.2f} | Loss: {loss:.4f} | Epsilon: {agent.epsilon:.3f}\")\n",
        "\n",
        "    print(f\"Training finished. Best average reward: {best_avg_reward:.2f} saved to {save_path}\")\n",
        "    env.close()\n",
        "    return agent, episode_rewards"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kt4lDsWa6Ozp"
      },
      "source": [
        "\n",
        "1.  **Initialization:**\n",
        "    *   Creates the `CartPole-v1` environment using `gym.make`.\n",
        "    *   Gets state and action space sizes from the environment.\n",
        "    *   Instantiates the policy gradient agent (`PolicyGradientAgentDiscrete`).\n",
        "    *   Sets up lists/variables for tracking rewards (`episode_rewards`, `recent_rewards`) and saving the best model (`best_avg_reward`, `save_path`).\n",
        "2.  **Episode Loop:** Runs for the specified number of `episodes`.\n",
        "    *   **Reset:** Resets the environment (`env.reset()`) to get the initial state. Initializes lists (`states_memory`, `actions_memory`, `rewards_memory`) to store the trajectory for the current episode.\n",
        "    *   **Step Loop:** Runs for a maximum number of steps (`env.spec.max_episode_steps`).\n",
        "        *   Agent selects an `action` based on the current `state` using its policy network (`agent.get_action`).\n",
        "        *   Environment performs the action (`env.step(action)`), returning `next_state`, `reward`, `terminated`, `truncated`.\n",
        "        *   Stores the `state`, `action`, and `reward` in the memory lists.\n",
        "        *   Updates the current `state` and `total_reward`.\n",
        "        *   Breaks the loop if the episode ends (`terminated` or `truncated`).\n",
        "    *   **Agent Training:** After the episode finishes, calls `agent.train()` with the collected states, actions, and rewards for that episode. This is the **Monte Carlo** aspect – updating only after the full episode outcome is known.\n",
        "    *   **Performance Tracking:** Updates `episode_rewards` and the `recent_rewards` window. Calculates `current_avg_reward`.\n",
        "    *   **Model Saving:** If `current_avg_reward` is the best seen so far, saves the agent's network weights (`agent.state_dict()`) to `save_path`.\n",
        "    *   **Logging:** Prints progress periodically.\n",
        "3.  **Cleanup:** Closes the environment (`env.close()`). Returns the trained agent and the history of episode rewards.\n",
        "\n",
        "### Implementing the DQN Agent (`DQNAgent`)\n",
        "\n",
        "Now let's define the agent class, which includes the Q-network, target network, and replay buffer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "zs5Srvy8IGZO"
      },
      "outputs": [],
      "source": [
        "class DQNAgent(nn.Module):\n",
        "    def __init__(self, state_size, action_size, hidden_size, lr, buffer_size=10000, batch_size=64, gamma=0.99, tau=0.005, epsilon_start=1.0, epsilon_end=0.01, epsilon_decay=0.995):\n",
        "        super().__init__()\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.lr = lr\n",
        "        self.gamma = gamma\n",
        "        self.tau = tau\n",
        "        self.epsilon = epsilon_start\n",
        "        self.epsilon_end = epsilon_end\n",
        "        self.epsilon_decay = epsilon_decay\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        # Q-network\n",
        "        self.q_network = nn.Sequential(\n",
        "            nn.Linear(state_size, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, action_size)\n",
        "        )\n",
        "\n",
        "        # Target network\n",
        "        self.target_network = nn.Sequential(\n",
        "            nn.Linear(state_size, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, action_size)\n",
        "        )\n",
        "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
        "        self.target_network.eval()\n",
        "\n",
        "        # Optimizer\n",
        "        self.optimizer = optim.AdamW(self.q_network.parameters(), lr=lr)\n",
        "\n",
        "        # Replay buffer\n",
        "        self.buffer = deque(maxlen=buffer_size)\n",
        "\n",
        "    def get_action(self, state):\n",
        "        if np.random.rand() < self.epsilon:\n",
        "            return np.random.randint(self.action_size)\n",
        "        else:\n",
        "            state_tensor = torch.tensor(state, dtype=torch.float32).to(device).unsqueeze(0)\n",
        "            with torch.no_grad():\n",
        "                q_values = self.q_network(state_tensor)\n",
        "                return q_values.argmax().item()\n",
        "\n",
        "    def store_experience(self, state, action, reward, next_state, done):\n",
        "        self.buffer.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def train(self):\n",
        "        if len(self.buffer) < self.batch_size:\n",
        "            return 0.0\n",
        "\n",
        "        # Sample batch\n",
        "        batch = np.random.choice(len(self.buffer), self.batch_size, replace=False)\n",
        "        states, actions, rewards, next_states, dones = [], [], [], [], []\n",
        "\n",
        "        for idx in batch:\n",
        "            s, a, r, ns, d = self.buffer[idx]\n",
        "            states.append(s)\n",
        "            actions.append(a)\n",
        "            rewards.append(r)\n",
        "            next_states.append(ns)\n",
        "            dones.append(d)\n",
        "\n",
        "        states = torch.tensor(states, dtype=torch.float32).to(device)\n",
        "        actions = torch.tensor(actions, dtype=torch.long).to(device)\n",
        "        rewards = torch.tensor(rewards, dtype=torch.float32).to(device)\n",
        "        next_states = torch.tensor(next_states, dtype=torch.float32).to(device)\n",
        "        dones = torch.tensor(dones, dtype=torch.float32).to(device)\n",
        "\n",
        "        # Compute Q targets\n",
        "        with torch.no_grad():\n",
        "            next_q_values = self.target_network(next_states)\n",
        "            max_next_q = next_q_values.max(1)[0]\n",
        "            targets = rewards + self.gamma * max_next_q * (1 - dones)\n",
        "\n",
        "        # Compute current Q values\n",
        "        current_q_values = self.q_network(states)\n",
        "        current_q = current_q_values.gather(1, actions.unsqueeze(1)).squeeze(1)\n",
        "\n",
        "        # Compute loss\n",
        "        loss = F.mse_loss(current_q, targets)\n",
        "\n",
        "        # Update\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        # Update target network\n",
        "        for target_param, param in zip(self.target_network.parameters(), self.q_network.parameters()):\n",
        "            target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
        "\n",
        "        # Decay epsilon\n",
        "        self.epsilon = max(self.epsilon_end, self.epsilon * self.epsilon_decay)\n",
        "\n",
        "        return loss.item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "pH7R8-731DpT",
        "outputId": "e81d2478-81f7-4999-a217-edb6dc1bcad9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Training DQN Agent on CartPole-v1:\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'deque' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2993719841.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;31m# Call the training function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m trained_agent_reinforce, rewards_history_reinforce = train_agent_cartpole(\n\u001b[0m\u001b[1;32m     33\u001b[0m     \u001b[0mepisodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1500\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mhidden_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1015836258.py\u001b[0m in \u001b[0;36mtrain_agent_cartpole\u001b[0;34m(episodes, hidden_size, lr, gamma, print_every, save_path)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m# Instantiate agent and move to device\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDQNAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtau\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.005\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon_start\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon_end\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.995\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mepisode_rewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1512398880.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, state_size, action_size, hidden_size, lr, buffer_size, batch_size, gamma, tau, epsilon_start, epsilon_end, epsilon_decay)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;31m# Replay buffer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeque\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaxlen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbuffer_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'deque' is not defined"
          ]
        }
      ],
      "source": [
        "def plot_rewards(rewards, window=100):\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.plot(rewards, label='Episode Reward', alpha=0.6)\n",
        "    if len(rewards) >= window:\n",
        "        # Use numpy convolve for moving average\n",
        "        moving_avg = np.convolve(np.array(rewards), np.ones(window)/window, mode='valid')\n",
        "        plt.plot(np.arange(window-1, len(rewards)), moving_avg, label=f'Moving Average ({window} episodes)', color='red', linewidth=2)\n",
        "    plt.xlabel('Episode')\n",
        "    plt.ylabel('Total Reward')\n",
        "    plt.title('DQN on CartPole-v1: Episode Rewards')\n",
        "    # Get reward threshold from Gymnasium spec\n",
        "    reward_threshold = gym.spec(\"CartPole-v1\").reward_threshold\n",
        "    # Check if threshold is meaningful relative to plot limits before drawing\n",
        "    if reward_threshold is not None and plt.ylim()[1] > reward_threshold * 0.8 :\n",
        "        plt.hlines(reward_threshold, 0, len(rewards), colors='g', linestyles='dashed', label=f'Solved Threshold ({reward_threshold})')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "# --- Execution Logic ---\n",
        "# Define device (should be defined globally or re-defined here)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Set seed for reproducibility\n",
        "torch.manual_seed(42)\n",
        "\n",
        "print(\"Training DQN Agent on CartPole-v1:\")\n",
        "save_file_path = \"best_dqn_cartpole.pth\"\n",
        "\n",
        "# Call the training function\n",
        "trained_agent_reinforce, rewards_history_reinforce = train_agent_cartpole(\n",
        "    episodes=1500,\n",
        "    hidden_size=64,\n",
        "    lr=2e-3,\n",
        "    gamma=0.99,\n",
        "    print_every=50,\n",
        "    save_path=save_file_path\n",
        ")\n",
        "\n",
        "# Plot the results\n",
        "plot_rewards(rewards_history_reinforce)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EJ_KKLUW11G3"
      },
      "source": [
        "### Analyzing DQN Performance\n",
        "\n",
        "Alright! So, we ran our code using DQN for 1500 episodes. Let's check out the plot showing how it did!\n",
        "\n",
        "*   **Raw Rewards (Blue Line):** Shows the reward for each episode.\n",
        "*   **Moving Average (Red Line):** Smooths out the noise and shows the learning trend.\n",
        "*   **Success:** The agent learns to balance the pole using Q-learning with experience replay.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DzA436YH2LB0"
      },
      "source": [
        "<!-- Code Cell Output: A window should pop up showing the CartPole environment being controlled by the trained agent for a few episodes. -->\n",
        "\n",
        "#### Observing Agent Behavior\n",
        "\n",
        "You might notice interesting variations in the agent's behavior during rendering, even if it achieves the maximum reward:\n",
        "*   Sometimes the cart might move rapidly back and forth (jittery).\n",
        "*   Other times, it might stay relatively still with only small adjustments.\n",
        "\n",
        "This highlights that the agent learns purely based on the reward signal. The CartPole reward is simple: +1 for every step the pole is balanced. It doesn't penalize jerky movements or reward smooth control.\n",
        "\n",
        "Therefore, the agent finds *any* strategy that maximizes survival time, regardless of aesthetics or efficiency. If rapid adjustments keep the pole up for 500 steps, that's considered optimal by the agent according to the given reward function. To encourage smoother control, the reward function itself would need to be modified (e.g., adding penalties for large actions or high velocities).\n",
        "\n",
        "---\n",
        "\n",
        "### DQN Recap and Next Steps\n",
        "\n",
        "Okay, so DQN got the job done on CartPole! It learned to balance the pole using Q-learning with experience replay.\n",
        "\n",
        "**Advantages of DQN:**\n",
        "*   **Stable Learning:** Experience replay and target network reduce variance.\n",
        "*   **Off-Policy:** Can learn from past experiences, more sample-efficient.\n",
        "*   **Handles Discrete Actions:** Well-suited for environments with discrete action spaces.\n",
        "\n",
        "**Disadvantages of DQN:**\n",
        "*   **Hyperparameter Sensitive:** Requires tuning of buffer size, batch size, epsilon decay, etc.\n",
        "*   **Struggles with Continuous Actions:** Needs modifications for continuous action spaces.\n",
        "\n",
        "---\n",
        "\n",
        "### Homework: Advanced DQN Experiments\n",
        "\n",
        "**Objective:** Extend your understanding of DQN by testing it on different environments and optimizing hyperparameters.\n",
        "\n",
        "**Tasks:**\n",
        "\n",
        "1. **Test on Another Gym Environment:**\n",
        "   - Choose a different Gymnasium environment (e.g., `MountainCar-v0`, `Acrobot-v1`, `LunarLander-v2`).\n",
        "   - Adapt the DQN agent if needed (e.g., different state/action sizes).\n",
        "   - Train the agent and evaluate its performance.\n",
        "   - Compare results with CartPole.\n",
        "\n",
        "2. **Hyperparameter Optimization:**\n",
        "   - Experiment with different hyperparameters: At least 5 combinations (if it takes too long).\n",
        "     - Learning rate (lr): Try 1e-4, 5e-4, 1e-3, 5e-3\n",
        "     - Hidden layer size: Try 32, 64, 128, 256\n",
        "     - Batch size: Try 32, 64, 128\n",
        "     - Buffer size: Try 5000, 10000, 20000\n",
        "     - Epsilon decay: Try 0.99, 0.995, 0.999\n",
        "     - Gamma: Try 0.9, 0.95, 0.99, 0.999\n",
        "   - Use a systematic approach (e.g., grid search or random search) to find better combinations.\n",
        "   - Compare the best configuration with the default one.\n",
        "\n",
        "3. **Analysis and Reporting:**\n",
        "   - Plot learning curves for different hyperparameter settings.\n",
        "   - Explain the impact of each hyperparameter on performance:\n",
        "     - **Learning Rate:** Controls step size in gradient updates. Too high → unstable, too low → slow convergence.\n",
        "     - **Hidden Size:** Network capacity. Larger → can learn complex functions, but risk overfitting.\n",
        "     - **Batch Size:** Number of samples per update. Larger → more stable gradients, but slower updates.\n",
        "     - **Buffer Size:** Experience memory. Larger → more diverse samples, but older experiences.\n",
        "     - **Epsilon Decay:** Exploration rate reduction. Faster decay → exploit sooner, slower → explore longer.\n",
        "     - **Gamma:** Discount factor. Higher → values future rewards more, lower → focuses on immediate rewards.\n",
        "   - Discuss which hyperparameters had the biggest impact and why.\n",
        "\n",
        "**Submission:** Create a notebook or report showing your experiments, results, and explanations.\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "myenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}